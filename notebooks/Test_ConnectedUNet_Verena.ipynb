{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba740608-d2ce-48ba-a980-66c9105f2536",
   "metadata": {},
   "source": [
    "# Test Basic U-Net from Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7c8850-479d-42ab-bb79-e9b4445ba103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from csv_Extractor_Rohan import Extractor\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "filepath = 'C:/Users/veren/Documents/TechLab/breast-cancer-detection-project/data/processed/csv/'\n",
    "#extract = Extractor(filepath)\n",
    "#dict_keys = extract.get_file_names()\n",
    "#print(dict_keys)\n",
    "\n",
    "base_path =  'C:/Users/veren/Documents/TechLab/breast-cancer-detection-project/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b025e47-258c-428c-942f-8da41ccfb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the datasets.\n"
     ]
    }
   ],
   "source": [
    "# --- File Paths ---\n",
    "mass_train_path = os.path.join(filepath,'mass_final_dataset.csv')\n",
    "calc_train_path = os.path.join(filepath,'calc_final_dataset.csv')\n",
    "\n",
    "# --- Load the Datasets ---\n",
    "try:\n",
    "    mass_train_df = pd.read_csv(mass_train_path)\n",
    "    calc_train_df = pd.read_csv(calc_train_path)\n",
    "    print(\"Successfully loaded the datasets.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease ensure the filepath is correct and the CSV files exist in that directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25862449-fb39-4d2a-b233-1141049e3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into img and msk\n",
    "imgs_train1 = mass_train_df['full_image_path']\n",
    "imgs_mask_train1 = mass_train_df['roi_path']\n",
    "imgs_train2 = calc_train_df['full_image_path']\n",
    "imgs_mask_train2 = calc_train_df['roi_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51cad2b-28f3-4a99-b49a-a4931d4e145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for row in imgs_train1:\n",
    "    if os.path.exists(base_path + row):  # prüfen, ob Datei existiert\n",
    "        img_train3 = Image.open(base_path + row).convert(\"RGB\")   # Bild öffnen und in RGB konvertieren\n",
    "        img_train3 = img_train3.resize((256, 256))  \n",
    "        \n",
    "        # In NumPy-Array umwandeln\n",
    "        img_array = np.array(img_train3, dtype=np.float32)\n",
    "        \n",
    "        # Dimension für Kanal hinzufügen -> (256, 256, 1)\n",
    "        img_array = np.expand_dims(img_array, axis=-1)\n",
    "        #img_array = np.array(img_train3)               # in NumPy-Array umwandeln\n",
    "        images.append(img_array)\n",
    "    else:\n",
    "        print(f\"Datei nicht gefunden: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07924479-79d3-43a0-af4a-f2ecaa9d2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = []\n",
    "for row1 in imgs_mask_train1:\n",
    "    if os.path.exists(base_path + row1):  # prüfen, ob Datei existiert\n",
    "        img_mask_train3 = Image.open(base_path + row1).convert(\"RGB\")   # Bild öffnen und in RGB konvertieren\n",
    "        img_mak_train3 = img_mask_train3.resize((256, 256))  \n",
    "        \n",
    "        # In NumPy-Array umwandeln\n",
    "        img_array = np.array(img_mak_train3, dtype=np.float32)\n",
    "        \n",
    "        # Dimension für Kanal hinzufügen -> (256, 256, 1)\n",
    "        img_array = np.expand_dims(img_array, axis=-1)\n",
    "        #img_array = np.array(img_mask_train3)               # in NumPy-Array umwandeln\n",
    "        mask.append(img_array)\n",
    "    else:\n",
    "        print(f\"Datei nicht gefunden: {base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4a85cd-b40a-4233-94e7-356345ade04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "images_train, images_val, images_mask_train, imagess_mask_val = train_test_split(images, mask, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "586e576c-4143-45a6-9df8-d5152eb98950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "#from skimage.io import imsave\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, Add, MaxPooling2D, Conv2DTranspose, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, Adadelta, Adagrad\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "#from data import load_train_data, load_test_data\n",
    "#from sklearn.model_selection import train_test_split\n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "\n",
    "\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
    "  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "  return iou\n",
    "\n",
    "def sens(y_true, y_pred):\n",
    "    num=K.sum(K.multiply(y_true, y_pred))\n",
    "    denom=K.sum(y_true)\n",
    "    if denom==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return  num/denom\n",
    "\n",
    "def spec(y_true, y_pred):\n",
    "    num=K.sum(K.multiply(y_true==0, y_pred==0))\n",
    "    denom=K.sum(y_true==0)\n",
    "    if denom==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return  num/denom\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return -(0.4*dice_coef(y_true, y_pred)+0.6*iou_coef(y_true, y_pred))\n",
    "\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    BCE = K.binary_crossentropy(y_true_f, y_pred_f)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(0.8 * K.pow((1-BCE_EXP), 2.) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "def tversky(y_true, y_pred):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true,y_pred)\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)\n",
    "\n",
    "\n",
    "def seg_loss(y_true, y_pred):\n",
    "    return -(0.4*dice_coef(y_true, y_pred)+0.6*iou_coef(y_true, y_pred))\n",
    "\n",
    "def get_unet():\n",
    "    inputs = Input((img_rows, img_cols, 3))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    \n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    \n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    \n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "        \n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "    model.compile(optimizer=Adam(), loss=[loss], metrics=[dice_coef, iou_coef])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebf87e5-e978-46d2-87d8-03abe1fa49b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFitting model...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m history = model.fit(imgs_train, imgs_mask_train,\n\u001b[32m     42\u001b[39m                     batch_size=\u001b[32m8\u001b[39m, epochs=\u001b[32m100\u001b[39m, verbose=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     43\u001b[39m                     validation_data=(imgs_val, imgs_mask_val),\n\u001b[32m     44\u001b[39m                     callbacks=[model_checkpoint])\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoading and preprocessing test data...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\techlab\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mloss\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(y_true, y_pred):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -(\u001b[32m0.4\u001b[39m*dice_coef(y_true, y_pred)+\u001b[32m0.6\u001b[39m*iou_coef(y_true, y_pred))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mdice_coef\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdice_coef\u001b[39m(y_true, y_pred):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     y_true_f = K.flatten(y_true)\n\u001b[32m     21\u001b[39m     y_pred_f = K.flatten(y_pred)\n\u001b[32m     22\u001b[39m     intersection = K.sum(y_true_f * y_pred_f)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'keras.backend' has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "#name = 'join_inbreast4'\n",
    "\n",
    "#imgs_train1, imgs_mask_train1 = load_train_data('inbreast_cycleGAN')\n",
    "#imgs_train2, imgs_mask_train2 = load_train_data('mydata')\n",
    "\n",
    "#imgs_train = np.concatenate([imgs_train1, imgs_train2])\n",
    "#imgs_mask_train = np.concatenate([imgs_mask_train1, imgs_mask_train2])\n",
    "imgs_train = np.stack(images_train)\n",
    "imgs_mask_train = np.stack(images_mask_train)\n",
    "#imgs_train, imgs_mask_train = load_train_data(name)\n",
    "name = 'cbis-ddsm'\n",
    "\n",
    "fname = 'basic_unet_'+name+'_weights.h5'\n",
    "pred_dir = fname[:-11]\n",
    "\n",
    "imgs_train = imgs_train.astype('float32')\n",
    "\n",
    "mean = np.mean(imgs_train)  # mean for data centering\n",
    "std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "imgs_train -= mean\n",
    "imgs_train /= std\n",
    "imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "\n",
    "imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "imgs_mask_train = imgs_mask_train[..., np.newaxis]\n",
    "\n",
    "imgs_train, imgs_val, imgs_mask_train, imgs_mask_val = train_test_split(imgs_train, imgs_mask_train, test_size=0.2, random_state=42)\n",
    "print('-'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('-'*30)\n",
    "\n",
    "model = get_unet()\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(fname, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "print('-'*30)\n",
    "print('Fitting model...')\n",
    "print('-'*30)\n",
    "\n",
    "history = model.fit(imgs_train, imgs_mask_train,\n",
    "                    batch_size=8, epochs=100, verbose=1, shuffle=True,\n",
    "                    validation_data=(imgs_val, imgs_mask_val),\n",
    "                    callbacks=[model_checkpoint])\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading and preprocessing test data...')\n",
    "print('-'*30)\n",
    "\n",
    "#imgs_test1, imgs_id_test1 = load_test_data('inbreast_cycleGAN')\n",
    "#imgs_test2, imgs_id_test2 = load_test_data('mydata')\n",
    "\n",
    "#imgs_test = np.concatenate([imgs_test1, imgs_test2])\n",
    "#imgs_id_test = np.concatenate([imgs_id_test1, imgs_id_test2])\n",
    "\n",
    "#imgs_test, imgs_id_test = load_test_data(name)\n",
    "imgs_train = np.stack(images_val)\n",
    "imgs_mask_train = np.stack(images_mask_val)\n",
    "\n",
    "imgs_test = imgs_test.astype('float32')\n",
    "imgs_test -= mean\n",
    "imgs_test /= std\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading saved weights...')\n",
    "print('-'*30)\n",
    "model.load_weights(fname)\n",
    "\n",
    "print('-'*30)\n",
    "print('Predicting masks on test data...')\n",
    "print('-'*30)\n",
    "imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "np.save('imgs_mask_test_'+name+'_basic_unet.npy', imgs_mask_test)\n",
    "\n",
    "\n",
    "print('-' * 30)\n",
    "print('Saving predicted masks to files...')\n",
    "print('-' * 30)\n",
    "\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.mkdir(pred_dir)\n",
    "\n",
    "#data_path2 = 'D:/Files/MYDATA/Breast_Cancer-Begonya/Images/Test_Seg/'\n",
    "#data_path2 = 'D:/INbreast/Test_Seg/'\n",
    "#data_path2 = 'D:/CBIS_augmented/Test_Seg/'\n",
    "#data_path2 = 'D:/CSAW-S/CsawS/Test_Seg/'\n",
    "\n",
    "#d = data_path2+'roi/*.png'    \n",
    "#files = glob.glob(d) \n",
    "\n",
    "#files1 = files\n",
    "\n",
    "#data_path2 = 'D:/Files/MYDATA/Breast_Cancer-Begonya/Images/Test_Seg/'\n",
    "\n",
    "#d = data_path2+'roi/*.png'    \n",
    "#files = glob.glob(d) \n",
    "\n",
    "#files2 = files\n",
    "\n",
    "#files = files1 + files2\n",
    "\n",
    "\n",
    "#files = [file.split('\\\\')[-1][:-4] for file in files]\n",
    "#idx = 0\n",
    "#for image, image_id in zip(imgs_mask_test, imgs_id_test):\n",
    "#    image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
    "#    imsave(os.path.join(pred_dir, files[idx] + '_pred.png'), image)\n",
    "#    idx = idx + 1\n",
    "\n",
    "imgs_id_test = imgs_id_test.astype('float32')\n",
    "imgs_id_test = imgs_id_test[..., np.newaxis]\n",
    "imgs_id_test = imgs_id_test // 255\n",
    "\n",
    "ev = model.evaluate(imgs_test, imgs_id_test)\n",
    "dice, iou = ev[1], ev[2]\n",
    "\n",
    "print(\"dice score:\", dice)\n",
    "print(\"iou score:\", iou)\n",
    "\n",
    "   \n",
    "plt.plot(history.history['dice_coef'])\n",
    "plt.plot(history.history['val_dice_coef'])\n",
    "plt.title('model dice coef')\n",
    "plt.ylabel('dice coef')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['iou_coef'])\n",
    "plt.plot(history.history['val_iou_coef'])\n",
    "plt.title('model iou coef')\n",
    "plt.ylabel('iou coef')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d5c6a-9322-42fd-a81e-1fe4a8211e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
